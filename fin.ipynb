{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Nixon Showalter\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Imports for data analysis and other fun things :3\n",
    "import kagglehub\n",
    "import pandas as pd\n",
    "import ast\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from transformers import DistilBertTokenizer, DistilBertForSequenceClassification, DistilBertConfig, TrainingArguments, Trainer\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Path to dataset files: C:\\Users\\Nixon Showalter\\.cache\\kagglehub\\datasets\\mobasshir\\yelpdata\\versions\\2\n"
     ]
    }
   ],
   "source": [
    "# Get dataset from Kaggle downloaded on any machine\n",
    "path = kagglehub.dataset_download(\"mobasshir/yelpdata\")\n",
    "\n",
    "print(\"Path to dataset files:\", path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the two required data sets into variables\n",
    "bus_df = pd.read_csv(f\"{path}/yelp_business.csv\")\n",
    "review_df = pd.read_csv(f\"{path}/yelp_review_arizona.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['business_id', 'name', 'address', 'city', 'state', 'postal_code',\n",
      "       'latitude', 'longitude', 'stars', 'review_count', 'is_open',\n",
      "       'attributes', 'categories', 'hours'],\n",
      "      dtype='object')\n",
      "Index(['review_id', 'user_id', 'business_id', 'text', 'stars', 'date'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# Display the column names of each set\n",
    "print(bus_df.columns)\n",
    "print(review_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Business Data Pre-Cleaning:  (192609, 14)\n",
      "Business Data Post-Cleaning:  (26524, 14)\n"
     ]
    }
   ],
   "source": [
    "# Clean the business data to the required specifications\n",
    "print(\"Business Data Pre-Cleaning: \", bus_df.shape)\n",
    "\n",
    "clean_bus_df = bus_df[\n",
    "    (bus_df['review_count'] > 50) &\n",
    "    (bus_df['attributes'].size > 3)\n",
    "]\n",
    "\n",
    "print(\"Business Data Post-Cleaning: \", clean_bus_df.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge all of the reviews for each business into one column\n",
    "per_bus_reviews = review_df.groupby('business_id')['text'].apply(lambda text: \" \".join(text))\n",
    "\n",
    "# Merges the business and review data sets into one\n",
    "merged_df = pd.merge(clean_bus_df, per_bus_reviews, on='business_id', how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique attributes: ['AgesAllowed', 'Alcohol', 'Ambience', 'BYOB', 'BYOBCorkage', 'BestNights', 'BikeParking', 'BusinessAcceptsBitcoin', 'BusinessAcceptsCreditCards', 'BusinessParking', 'ByAppointmentOnly', 'Caters', 'CoatCheck', 'Corkage', 'DietaryRestrictions', 'DogsAllowed', 'DriveThru', 'GoodForDancing', 'GoodForKids', 'GoodForMeal', 'HappyHour', 'HasTV', 'Music', 'NoiseLevel', 'Open24Hours', 'OutdoorSeating', 'RestaurantsAttire', 'RestaurantsCounterService', 'RestaurantsDelivery', 'RestaurantsGoodForGroups', 'RestaurantsPriceRange2', 'RestaurantsReservations', 'RestaurantsTableService', 'RestaurantsTakeOut', 'Smoking', 'WheelchairAccessible', 'WiFi']\n"
     ]
    }
   ],
   "source": [
    "unique_keys = []\n",
    "target_labels = []\n",
    "\n",
    "# Changes the attributes category from a string that looks like a dict to a dict\n",
    "bus_attributes = merged_df['attributes'].apply(lambda x: eval(x))\n",
    "\n",
    "\n",
    "# Creates a list of all of the unique attributes a business can have\n",
    "for attributes_dict in bus_attributes:\n",
    "    for key in attributes_dict.keys():\n",
    "        if key not in unique_keys:\n",
    "            unique_keys.append(key)\n",
    "\n",
    "unique_attributes = sorted(unique_keys)\n",
    "\n",
    "print(\"Unique attributes:\", unique_attributes)\n",
    "\n",
    "# Creates a list of all of the labels each business has seperately\n",
    "for attributes_dict in bus_attributes:\n",
    "    current_labels = []\n",
    "    for attribute in unique_attributes:\n",
    "        if attribute in attributes_dict:\n",
    "            current_labels.append(attribute)\n",
    "    target_labels.append(current_labels)\n",
    "\n",
    "# Preps the multilabel binarizer and the fit labels for the BERT model\n",
    "mlb = MultiLabelBinarizer(classes=unique_attributes)\n",
    "labels = mlb.fit_transform(target_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splits the testing and training data\n",
    "train_texts, test_texts, y_train, y_test = train_test_split(merged_df['text'], labels, test_size=0.2, random_state=9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Generates the tokenizer, config, and model using DistilBert\n",
    "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "\n",
    "config = DistilBertConfig.from_pretrained('distilbert-base-uncased', num_labels=len(unique_attributes), problem_type=\"multi_label_classification\")\n",
    "\n",
    "model = DistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased', config=config)\n",
    "\n",
    "# Generates the encodings for both the training and testing data\n",
    "# Hyperparams set to avoid throwing errors when using the TensorDataset\n",
    "train_encodings = tokenizer(train_texts.tolist(), truncation=True, padding='max_length', max_length=256,return_tensors='pt')\n",
    "\n",
    "test_encodings = tokenizer(test_texts.tolist(), truncation=True, padding='max_length', max_length=256, return_tensors='pt')\n",
    "\n",
    "# Gets the tensor from torch to create the dataset that the model will train on\n",
    "# Torch float changes output type for model to cast as a Float later\n",
    "train_labels_tensor = torch.tensor(y_train, dtype=torch.float)\n",
    "test_labels_tensor = torch.tensor(y_test, dtype=torch.float)\n",
    "\n",
    "# Creates the training and testing datasets that can be used with DistilBERT\n",
    "train_dataset = TensorDataset(train_encodings['input_ids'], train_encodings['attention_mask'], train_labels_tensor)\n",
    "test_dataset = TensorDataset(test_encodings['input_ids'], test_encodings['attention_mask'], test_labels_tensor)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates the list of training arguments, using just the bare minimum arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./distilbert_results',\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16\n",
    ")\n",
    "\n",
    "# Creates a function that will collect the feature outputs from the model\n",
    "# Used to solve error where training the model would throw a value error\n",
    "def data_collator(features):\n",
    "    data = {}\n",
    "    data['input_ids'] = torch.stack([f[0] for f in features])\n",
    "    data['attention_mask'] = torch.stack([f[1] for f in features])\n",
    "    data['labels'] = torch.stack([f[2] for f in features])\n",
    "    \n",
    "    return data\n",
    "\n",
    "# Creates the list of arguments for the trainer utilizing the variables created above\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    "    data_collator=data_collator\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='213' max='213' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [213/213 37:50, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='18' max='18' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [18/18 00:43]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results: {'eval_loss': 0.238334521651268, 'eval_runtime': 46.2054, 'eval_samples_per_second': 6.125, 'eval_steps_per_second': 0.39, 'epoch': 3.0}\n"
     ]
    }
   ],
   "source": [
    "# Trains and evaluates the model, printing out the results\n",
    "trainer.train()\n",
    "eval_results = trainer.evaluate()\n",
    "print(\"Results:\", eval_results)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
